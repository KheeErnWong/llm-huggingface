{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92eeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import make_classification\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d647d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 90\n",
      "Start value: $100,000.00\n",
      "End value: $90,019.86\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portfolio_date</th>\n",
       "      <th>total_portfolio_value_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>99842.603548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>100862.528499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-06</td>\n",
       "      <td>103217.209398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-07</td>\n",
       "      <td>102906.288134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  portfolio_date  total_portfolio_value_usd\n",
       "0     2025-01-01              100000.000000\n",
       "1     2025-01-02               99842.603548\n",
       "2     2025-01-03              100862.528499\n",
       "3     2025-01-06              103217.209398\n",
       "4     2025-01-07              102906.288134"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Generate 90 business days\n",
    "start_date = \"2025-01-01\"\n",
    "date_range = pd.date_range(start=start_date, periods=90, freq=\"B\")\n",
    "\n",
    "# Simulate realistic portfolio with drift and volatility\n",
    "initial_value = 100000\n",
    "daily_returns = np.random.normal(\n",
    "    0.0005, 0.015, size=len(date_range)\n",
    ")  # 0.05% drift, 1.5% daily vol\n",
    "\n",
    "portfolio_values = [initial_value]\n",
    "for ret in daily_returns[1:]:\n",
    "    portfolio_values.append(portfolio_values[-1] * (1 + ret))\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"portfolio_date\": date_range, \"total_portfolio_value_usd\": portfolio_values}\n",
    ")\n",
    "\n",
    "print(f\"Length: {len(df)}\")\n",
    "print(f\"Start value: ${df['total_portfolio_value_usd'].iloc[0]:,.2f}\")\n",
    "print(f\"End value: ${df['total_portfolio_value_usd'].iloc[-1]:,.2f}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08b8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.2089527015353037\n"
     ]
    }
   ],
   "source": [
    "df[\"daily_returns\"] = df[\"total_portfolio_value_usd\"].pct_change()\n",
    "risk_free_rate = 0.03\n",
    "mean_returns = df[\"daily_returns\"].mean()\n",
    "std_dev_daily = df[\"daily_returns\"].std()\n",
    "annualized_volatility = std_dev_daily * np.sqrt(252)\n",
    "sharpe_ratio = (mean_returns - risk_free_rate) / std_dev_daily\n",
    "peak, trough = df[\"daily_returns\"].max(), df[\"daily_returns\"].min()\n",
    "max_drawdown = (trough - peak) / peak * 100\n",
    "var_95 = df[\"daily_returns\"].quantile(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088864",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 20\n",
    "assert (\n",
    "    df[\"daily_returns_centered\"].iloc[index]\n",
    "    == df[\"daily_returns\"].iloc[index] - mean_returns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e1430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f018c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47d60900",
   "metadata": {},
   "source": [
    "# Pluang portfolio simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b90ae1",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- User accounts\n",
    "CREATE TABLE users (\n",
    "    user_id INT PRIMARY KEY,\n",
    "    email VARCHAR(255),\n",
    "    created_at TIMESTAMP,\n",
    "    country_code VARCHAR(2),\n",
    "    kyc_status VARCHAR(20)\n",
    ");\n",
    "\n",
    "-- Asset types available on platform\n",
    "CREATE TABLE assets (\n",
    "    asset_id INT PRIMARY KEY,\n",
    "    asset_code VARCHAR(10),  -- e.g., 'AAPL', 'GOLD', 'BTC'\n",
    "    asset_name VARCHAR(100),\n",
    "    asset_type VARCHAR(20),  -- 'stock', 'commodity', 'crypto'\n",
    "    currency VARCHAR(3)      -- 'USD', 'IDR', etc.\n",
    ");\n",
    "\n",
    "-- User transactions (buys and sells)\n",
    "CREATE TABLE transactions (\n",
    "    transaction_id INT PRIMARY KEY,\n",
    "    user_id INT,\n",
    "    asset_id INT,\n",
    "    transaction_type VARCHAR(4),  -- 'buy' or 'sell'\n",
    "    quantity DECIMAL(18, 8),\n",
    "    price_per_unit DECIMAL(18, 4),  -- in asset currency\n",
    "    transaction_date TIMESTAMP,\n",
    "    fee_amount DECIMAL(18, 4),\n",
    "    FOREIGN KEY (user_id) REFERENCES users(user_id),\n",
    "    FOREIGN KEY (asset_id) REFERENCES assets(asset_id)\n",
    ");\n",
    "\n",
    "-- Daily closing prices for all assets\n",
    "CREATE TABLE daily_prices (\n",
    "    price_id INT PRIMARY KEY,\n",
    "    asset_id INT,\n",
    "    price_date DATE,\n",
    "    closing_price DECIMAL(18, 4),\n",
    "    opening_price DECIMAL(18, 4),\n",
    "    high_price DECIMAL(18, 4),\n",
    "    low_price DECIMAL(18, 4),\n",
    "    FOREIGN KEY (asset_id) REFERENCES assets(asset_id)\n",
    ");\n",
    "\n",
    "-- Exchange rates to convert everything to a base currency\n",
    "CREATE TABLE exchange_rates (\n",
    "    rate_id INT PRIMARY KEY,\n",
    "    from_currency VARCHAR(3),\n",
    "    to_currency VARCHAR(3),  -- Base currency is 'USD'\n",
    "    rate_date DATE,\n",
    "    exchange_rate DECIMAL(18, 6)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531dc3a",
   "metadata": {},
   "source": [
    "Task 1: Write a query to calculate each user's current portfolio\n",
    "  holdings (net position) for all assets. The result should include:\n",
    "  - user_id\n",
    "  - asset_code\n",
    "  - asset_type\n",
    "  - total_quantity_held (sum of buys minus sells)\n",
    "  - average_cost_basis (weighted average purchase price in USD)\n",
    "  - current_value_usd (using the most recent closing price converted to\n",
    "  USD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37885d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Task 1: Current Portfolio Holdings\n",
    "\n",
    "Logic breakdown:\n",
    "\n",
    "1. Calculate net positions per user-asset\n",
    "- Sum buy transactions (positive quantity)\n",
    "- Subtract sell transactions (negative quantity)\n",
    "- Group by user_id and asset_id\n",
    "2. Calculate weighted average cost basis\n",
    "- For buys only: sum(quantity × price) / sum(quantity)\n",
    "- Convert to USD using exchange rates at transaction dates\n",
    "3. Get current value\n",
    "- Join with most recent daily_prices closing_price\n",
    "- Convert to USD using latest exchange rate\n",
    "- Multiply by total_quantity_held\n",
    "4. Key joins needed:\n",
    "- transactions → assets (for asset details)\n",
    "- transactions → exchange_rates (for historical conversion)\n",
    "- assets → daily_prices (for current prices)\n",
    "- Filter where net quantity > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b810de",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH net_positions AS(\n",
    "    SELECT \n",
    "        t.user_id,\n",
    "        t.asset_id,\n",
    "        SUM(\n",
    "            CASE\n",
    "                WHEN t.transaction_type = 'buy' THEN t.quantity\n",
    "                WHEN t.transaction_type = 'sell' THEN -t.quantity\n",
    "            END \n",
    "        ) AS total_quantity_held,\n",
    "        SUM(\n",
    "            CASE\n",
    "                WHEN t.transaction_type = 'buy' THEN t.quantity * t.price_per_unit\n",
    "                ELSE 0\n",
    "            END\n",
    "        ) / NULLIF(\n",
    "            SUM(\n",
    "                CASE\n",
    "                    WHEN t.transaction_type = 'buy' THEN t.quantity\n",
    "                    ELSE 0\n",
    "                END\n",
    "            ), 0\n",
    "        ) AS average_cost_basis_local\n",
    "    FROM \n",
    "        transactions as t\n",
    "    GROUP BY\n",
    "        t.user_id,\n",
    "        t.asset_id\n",
    "    HAVING\n",
    "        SUM(CASE \n",
    "            WHEN t.transaction_type = 'buy' THEN t.quantity\n",
    "            WHEN t.transaction_type = 'sell' THEN -t.quantity \n",
    "        END) > 0\n",
    "),\n",
    "\n",
    "last_price AS(\n",
    "    SELECT\n",
    "        dp.asset_id,\n",
    "        dp.price_date,\n",
    "        dp.closing_price,\n",
    "        ROW_NUMBER() OVER(PARTITION BY dp.asset_id ORDER BY dp.price_date DESC) as rn\n",
    "    FROM\n",
    "        daily_prices as dp\n",
    "),\n",
    "\n",
    "currency_rates AS(\n",
    "    SELECT\n",
    "        er.from_currency,\n",
    "        er.to_currency,\n",
    "        er.rate_date,\n",
    "        er.exchange_rate,\n",
    "        ROW_NUMBER() OVER(PARTITION BY er.from_currency ORDER BY er.rate_date DESC) AS rn\n",
    "    FROM\n",
    "        exchange_rates as er\n",
    "    WHERE\n",
    "        er.to_currency = 'USD'\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    np.user_id,\n",
    "    a.asset_code,\n",
    "    a.asset_type,\n",
    "    np.total_quantity_held,\n",
    "    round(np.average_cost_basis_local * COALESCE(cr.exchange_rate,1), 4) AS average_cost_basis,\n",
    "    round(np.total_quantity_held * lp.closing_price * COALESCE(cr.exchange_rate,1), 4) AS current_value_usd\n",
    "FROM\n",
    "    net_positions AS np\n",
    "INNER JOIN\n",
    "    assets AS a ON np.asset_id = a.asset_id\n",
    "LEFT JOIN\n",
    "    currency_rates AS cr ON a.currency = cr.from_currency AND cr.rn = 1\n",
    "INNER JOIN\n",
    "    last_price AS lp ON np.asset_id = lp.asset_id AND lp.rn = 1\n",
    "ORDER BY\n",
    "    np.user_id,\n",
    "    a.asset_type,\n",
    "    a.asset_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe98674",
   "metadata": {},
   "source": [
    "# Simple rolling average and lagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT date, closing_price\n",
    "FROM StockPrices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end date\n",
    "start_date = date(2024, 1, 1)\n",
    "end_date = date(2024, 12, 31)\n",
    "\n",
    "# Get pandas series of date range\n",
    "date = pd.date_range(start=start_date, end=end_date, freq=\"B\")\n",
    "\n",
    "# Display date\n",
    "print(date)\n",
    "len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate closing price data\n",
    "np.random.seed = 42\n",
    "closing_price = np.random.uniform(100, 110, size=len(date))\n",
    "print(closing_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708080a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"date\": date, \"closing_price\": closing_price})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SMA_7_day\"] = df[\"closing_price\"].rolling(window=7).mean()\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc771023",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 13\n",
    "assert np.isclose(\n",
    "    df[\"closing_price\"].iloc[index - 6 : index + 1].mean(), df[\"SMA_7_day\"].iloc[index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lag_1\"] = df[\"closing_price\"].shift(-1)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5ecf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5014493b",
   "metadata": {},
   "source": [
    "# Find covariance of matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[1, 2, 3], [4, 5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50755a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean of each row\n",
    "mean = []\n",
    "\n",
    "# Change into list comprehension later\n",
    "for row in matrix:\n",
    "    mean_row = sum(row) / len(matrix[0])\n",
    "    mean.append(mean_row)\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da13ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center each row by subtracting it's mean\n",
    "centered = []\n",
    "\n",
    "for i in range(len(matrix)):\n",
    "    centered_row = []\n",
    "    for j in range(len(matrix[0])):\n",
    "        centered_row.append(matrix[i][j] - mean[i])\n",
    "\n",
    "    centered.append(centered_row)\n",
    "\n",
    "print(centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0055ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRY: Convert matrix[0] to a common variable\n",
    "\n",
    "multiplied = []\n",
    "for j in range(len(matrix[0])):\n",
    "    product = 1\n",
    "    sum = 0\n",
    "    for i in range(len(matrix)):\n",
    "        product *= centered[i][j]\n",
    "        sum += product\n",
    "    print(sum)\n",
    "\n",
    "print(multiplied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017295d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance\n",
    "covariance = sum(multiplied) / (len(multiplied) - 1)\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7821ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee9d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7d20bf",
   "metadata": {},
   "source": [
    "# Eigen values and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c378db",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[2, 1], [1, 2]]\n",
    "n = len(matrix)\n",
    "print(matrix)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity = []\n",
    "for i in range(n):\n",
    "    row = []\n",
    "    for j in range(n):\n",
    "        row.append(1 if i == j else 0)\n",
    "    identity.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca42f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1352f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use quadratic equation to solve for lambda (eigenvalue)\n",
    "## Store quadratic equation variables first\n",
    "a = 1\n",
    "b = -matrix[0][0] - matrix[1][1]\n",
    "c = matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "\n",
    "## Calc lambda\n",
    "lambda_1 = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n",
    "lambda_2 = (-b - np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n",
    "\n",
    "print(lambda_1)\n",
    "print(lambda_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = sorted([lambda_1, lambda_2], reverse=True)\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1e057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b594fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075937ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41065b54",
   "metadata": {},
   "source": [
    "# Multiply matrix by scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb20b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[1, 2, 3], [4, 5, 6]]\n",
    "scalar = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec68781",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(matrix)\n",
    "n_cols = len(matrix[0])\n",
    "print(n_rows)\n",
    "print(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        matrix[row][col] *= 2\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [1, 2, 3, 4]\n",
    "list = np.array(list)\n",
    "list *= 2\n",
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72036db",
   "metadata": {},
   "source": [
    "# Calculate mean y row/col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "mode = \"column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91281053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc mean by col\n",
    "n_cols = len(matrix[0])\n",
    "n_rows = len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc mean by row\n",
    "mean = [sum(row) / len(row) for row in matrix]\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95074e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc mean by col\n",
    "result = []\n",
    "\n",
    "mean = [\n",
    "    sum(matrix[row][col] for row in range(n_rows)) / n_rows for col in range(n_cols)\n",
    "]\n",
    "\n",
    "for col in range(n_cols):\n",
    "    column_sum = 0\n",
    "    for row in range(n_rows):\n",
    "        column_sum += matrix[row][col]\n",
    "    mean = column_sum / n_rows\n",
    "    result.append(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29e837",
   "metadata": {},
   "source": [
    "# Reshape matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba7b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2, 3, 4], [5, 6, 7, 8]]\n",
    "new_shape = (4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = [elem for row in a for elem in row]\n",
    "reshaped = [flat[i : i + 2] for i in range(0, len(flat), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ef876",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcbea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10ed26b5",
   "metadata": {},
   "source": [
    "# Transpose matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2740316",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2, 3], [4, 5, 6]]\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "tranposed = []\n",
    "\n",
    "transposed = [[a[row][col] for row in range(len(a))] for col in range(len(a[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a25478",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a16b5",
   "metadata": {},
   "source": [
    "# Matrix dot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 2], [2, 4]]\n",
    "b = [1, 2]\n",
    "\n",
    "result = np.dot(a, b)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = len(a)\n",
    "n_cols = len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_rows != n_cols\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for row in range(len(a)):\n",
    "    dot = sum(a[row][col] * b[col] for col in range(len(b)))\n",
    "    result.append(dot)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d7e53",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len: Max sequence length\n",
    "        d_model: Embedding dimension\n",
    "    Returns:\n",
    "        PE matrix (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    position = np.arange(seq_len).reshape(-1, 1)\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5788d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "d_model = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f15702",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = positional_encoding(10, 10)\n",
    "print(pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69662801",
   "metadata": {},
   "source": [
    "# Self attention mechanism implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the softmax using np.max() and not max()?\n",
    "# Give me the formula for softmax, and also why axis=-1 and keepdims=True\n",
    "# softmax(x) = np.exp(x)/sum(np.exp(x))\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, W_q, W_k, W_v):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: Input matrix (seq_len, d_model)\n",
    "        W_q: Query weight matrix (d_model, d_k)\n",
    "        W_k: Key weight matrix (d_model, d_k)\n",
    "        W_v: Value weight matrix (d_model, d_k)\n",
    "\n",
    "    Returns:\n",
    "        Output matrix (seq_len, d_v)\n",
    "    \"\"\"\n",
    "\n",
    "    # Project inputs to QKV\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "\n",
    "    # Calculate attention score\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.T / np.sqrt(d_k)  # (seq_len, seq_len)\n",
    "\n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)  # (seq_len, seq_len)\n",
    "\n",
    "    # Weighted sum of values\n",
    "    output = attention_weights @ V  # (seq_len, d_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e4254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, num_heads, W_q, W_k, W_v, W_o):\n",
    "    seq_len, d_model = X.shape\n",
    "    d_k = d_model // num_heads\n",
    "\n",
    "    # Project and split into heads\n",
    "    Q = (X @ W_q).reshape(seq_len, num_heads, d_k)\n",
    "    K = (X @ W_k).reshape(seq_len, num_heads, d_k)\n",
    "    V = (X @ W_v).reshape(seq_len, num_heads, d_k)\n",
    "\n",
    "    # Transpose for batche computation: (num_heads, seq_len, d_k)\n",
    "    Q = Q.transpose(1, 0, 2)\n",
    "    K = K.transpose(1, 0, 2)\n",
    "    V = V.transpose(1, 0, 2)\n",
    "\n",
    "    # Attention per head\n",
    "    scores = Q @ K.transpose(0, 2, 1) / np.sqrt(d_k)  # (num_heads, seq_len, seq_len)\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    head_outputs = attention_weights @ V  # (num_heads, seq_len, d_k)\n",
    "\n",
    "    # Concat heads and project\n",
    "    concat = head_outputs.transpose(1, 0, 2).reshape(\n",
    "        seq_len, d_model\n",
    "    )  # (seq_len, d_model)\n",
    "    output = concat @ W_o  # (seq_len, d_model). W_o (d_model, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee168f0",
   "metadata": {},
   "source": [
    "# Feature depency resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDependencyResolver:\n",
    "    def __init__(self, dependencies: dict):\n",
    "        # Build graph structure\n",
    "        pass\n",
    "\n",
    "    def topological_sort(self) -> list:\n",
    "        # Return valid computation order\n",
    "        pass\n",
    "\n",
    "    def has_cycle(self) -> bool:\n",
    "        # Return True if circular dependency exists\n",
    "        pass\n",
    "\n",
    "    def compute_parallel_batches(self) -> list:\n",
    "        # Return list of batches (each batch is a list of features)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb062dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    \"raw_price\": [],\n",
    "    \"raw_volume\": [],\n",
    "    \"returns\": [\"raw_price\"],\n",
    "    \"volatility\": [\"returns\"],\n",
    "    \"volume_ma\": [\"raw_volume\"],\n",
    "    \"price_volume_corr\": [\"returns\", \"volume_ma\"],\n",
    "    \"risk_score\": [\"volatility\", \"price_volume_corr\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build in-degree map\n",
    "## Build a new dict that has feature as key, and the no. of dependencies as value\n",
    "in_degree = {feature: len(deps) for feature, deps in dependencies.items()}\n",
    "print(in_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ecf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize queue with zero in-degree nodes\n",
    "queue = deque([feature for feature, degree in in_degree.items() if degree == 0])\n",
    "print(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "while queue:\n",
    "    current = queue.popleft()\n",
    "    result.append(current)\n",
    "\n",
    "    # Decrease in-degree for features depending on current\n",
    "    for feature, deps in dependencies.items():\n",
    "        if current in deps:\n",
    "            in_degree[feature] -= 1\n",
    "            if in_degree[feature] == 0:\n",
    "                queue.append(feature)\n",
    "\n",
    "# Check for cycle\n",
    "if len(result) != len(dependencies):\n",
    "    raise ValueError(\"Circular dependency detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ec7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc98855",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_degree = {feature: len(dependency) for feature, dependency in dependencies.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75223a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = deque(feature for feature, depdendency in in_degree.items() if depdendency == 0)\n",
    "queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ce82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c232ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "while queue:\n",
    "    current = queue.popleft()\n",
    "    if not dependencies[current]:\n",
    "        batch_num[current] = 0\n",
    "    else:\n",
    "        batch_num[current] = (\n",
    "            max(batch_num[dependency] for dependency in dependencies[current]) + 1\n",
    "        )\n",
    "\n",
    "    for feature, dependency in dependencies.items():\n",
    "        if current in dependency:\n",
    "            in_degree[feature] -= 1\n",
    "            if in_degree[feature] == 0:\n",
    "                queue.append(feature)\n",
    "\n",
    "max_batch = max(batch_num.values())\n",
    "batches = [[] for _ in range(max_batch + 1)]\n",
    "\n",
    "for feature, batch in batch_num.items():\n",
    "    batches[batch].append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611d15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_parallel_batches(dependencies: dict) -> list:\n",
    "    \"\"\"\n",
    "    Returns list of batches where each batch can execute in parallel.\n",
    "    Raises ValueError if circular dependency detected.\n",
    "    \"\"\"\n",
    "    # Step 1: Build reverse graph (who depends on me?)\n",
    "    graph = {feature: [] for feature in dependencies}\n",
    "    for feature, deps in dependencies.items():\n",
    "        for dep in deps:\n",
    "            graph[dep].append(feature)\n",
    "\n",
    "    # Step 2: Count incoming dependencies\n",
    "    in_degree = {feature: len(deps) for feature, deps in dependencies.items()}\n",
    "\n",
    "    # Step 3: Start with features that have no dependencies\n",
    "    queue = deque([f for f, degree in in_degree.items() if degree == 0])\n",
    "\n",
    "    # Step 4: Process in topological order, assign batch numbers\n",
    "    batch_num = {}\n",
    "\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "\n",
    "        # Assign batch: 0 if no deps, else max(dep batches) + 1\n",
    "        if len(dependencies[current]) == 0:\n",
    "            batch_num[current] = 0\n",
    "        else:\n",
    "            batch_num[current] = (\n",
    "                max(batch_num[dep] for dep in dependencies[current]) + 1\n",
    "            )\n",
    "\n",
    "        # Reduce in-degree for features that depend on current\n",
    "        for dependent in graph[current]:\n",
    "            in_degree[dependent] -= 1\n",
    "            if in_degree[dependent] == 0:\n",
    "                queue.append(dependent)\n",
    "\n",
    "    # Step 5: Check for cycles\n",
    "    if len(batch_num) != len(dependencies):\n",
    "        raise ValueError(\"Circular dependency detected\")\n",
    "\n",
    "    # Step 6: Group features by batch number\n",
    "    max_batch = max(batch_num.values())\n",
    "    batches = [[] for _ in range(max_batch + 1)]\n",
    "    for feature, batch in batch_num.items():\n",
    "        batches[batch].append(feature)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "def topological_sort(dependencies: dict) -> list:\n",
    "    \"\"\"Returns features in valid computation order.\"\"\"\n",
    "    batches = compute_parallel_batches(dependencies)\n",
    "    return [feature for batch in batches for feature in batch]\n",
    "\n",
    "\n",
    "def has_cycle(dependencies: dict) -> bool:\n",
    "    \"\"\"Returns True if circular dependency exists.\"\"\"\n",
    "    try:\n",
    "        compute_parallel_batches(dependencies)\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    \"raw_price\": [],\n",
    "    \"raw_volume\": [],\n",
    "    \"returns\": [\"raw_price\"],\n",
    "    \"volatility\": [\"returns\"],\n",
    "    \"volume_ma\": [\"raw_volume\"],\n",
    "    \"price_volume_corr\": [\"returns\", \"volume_ma\"],\n",
    "    \"risk_score\": [\"volatility\", \"price_volume_corr\"],\n",
    "}\n",
    "\n",
    "# Get parallel batches\n",
    "batches = compute_parallel_batches(dependencies)\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77833ba4",
   "metadata": {},
   "source": [
    "# EWM Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340265cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns, y = make_classification(n_features=4, random_state=42)\n",
    "print(returns.shape)\n",
    "print(returns[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179941da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewm_covariance(returns: np.ndarray, decay: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates exponentially weighted covariance matric\n",
    "    Args:\n",
    "        returns: Array of shape (n_days, n_assets) of daily returns\n",
    "        decay: Lambda parameter\n",
    "    Returns:\n",
    "        Covariance matrix\n",
    "    \"\"\"\n",
    "    # Get number of rows for weights calculation\n",
    "    n_days, n_assets = returns.shape\n",
    "\n",
    "    # Get exponents. Last row gets biggest number, oldest gets smallest\n",
    "    exponents = np.arange(n_days - 1, -1, -1)\n",
    "\n",
    "    # Calculate weights from exponents\n",
    "    weights = (1 - decay) * decay**exponents\n",
    "\n",
    "    # Normalize the weights\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    # Calculated weighted mean\n",
    "    weighted_mean = np.dot(weights, returns)\n",
    "\n",
    "    # Get centered value\n",
    "    centered = returns - weighted_mean\n",
    "\n",
    "    # Math trick to calculate covariance\n",
    "    ## Calculate sqrt weights\n",
    "    sqrt_weights = np.sqrt(weights).reshape(-1, 1)\n",
    "    ## Calculated centered_weighted\n",
    "    centered_weighted = centered * sqrt_weights\n",
    "    cov_matrix = centered_weighted.T @ centered_weighted\n",
    "\n",
    "    return cov_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76687393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = ewm_covariance(returns, decay=0.94)\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8f787",
   "metadata": {},
   "source": [
    "# Rolling risk metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"dates\": pd.date_range(\"2023-01-01\", periods=260, freq=\"B\"),\n",
    "        \"daily_return\": pd.Series(np.random.normal(0.001, 0.02, 260)),\n",
    "    }\n",
    ")\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"std_dev\"] = df[\"daily_return\"].rolling(window=20).std()\n",
    "df[\"volatility\"] = df[\"std_dev\"] * np.sqrt(252)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be76aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mean_return\"] = df[\"daily_return\"].rolling(window=20).mean()\n",
    "df[\"sharpe_ratio\"] = df[\"mean_return\"] / df[\"std_dev\"] * np.sqrt(252)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cumulative_wealth\"] = (1 + df[\"daily_return\"]).cumprod()\n",
    "df[\"rolling_peak\"] = df[\"cumulative_wealth\"].rolling(window=60).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7363574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd93116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max_drawdown\"] = df[\"drawdown\"].rolling(window=60).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f239204",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"max_drawdown\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c0836",
   "metadata": {},
   "source": [
    "# Feature Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66510b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1000, 50], [2000, 100], [1500, 75], [3000, 150]])\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cae20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransformer(ABC):\n",
    "    \"\"\"Abstract base class for all transformers\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"Learn parameters from X. Must be implemented by subclasses\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply transformatiion using learned parameters\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Convenience method: fir then transform\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820adca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Standard_Scaler(BaseTransformer):\n",
    "    \"\"\"Standardize features by remvoing mean andscalingto unit  variance\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute mean and std from training data\"\"\"\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Compute mean and std\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0)\n",
    "\n",
    "        # Handle edge case\n",
    "        self.std_ = np.where(self.std_ == 0, 1, self.std_)\n",
    "\n",
    "        return self  # Return self for method chaining\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"Apply standardization using store mean and std\"\n",
    "        # Check if fitted\n",
    "        if self.mean_ is None or self.std_ is None:\n",
    "            raise ValueError(\"StandardScaler is not fitted. Call fit() first\")\n",
    "        X = np.array(X)\n",
    "        return (X - self.mean_) / self.std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccaf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_Transformer(BaseTransformer):\n",
    "    \"\"\"Log transform doesn't need to fit anything, but we mark as fitted\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        # Check for negative values\n",
    "        if np.any(X < 0):\n",
    "            raise ValueError(\n",
    "                \"Data contains negative values. LogTransformer does not support negative values\"\n",
    "            )\n",
    "\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "\n",
    "        if not self.fitted_:\n",
    "            raise ValueError(\"Log_Transformer not fitted. Call fit() first\")\n",
    "\n",
    "        # Check for negative values\n",
    "        if np.any(X < 0):\n",
    "            raise ValueError(\n",
    "                \"Data contains negative values. LogTransformer does not support negative values\"\n",
    "            )\n",
    "\n",
    "        # Use log1p to handle 0 values better\n",
    "        return np.log1p(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Chain multiple transformers together\"\"\"\n",
    "\n",
    "    def __init__(self, transformers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transformers: List of transformer instances\n",
    "        \"\"\"\n",
    "        self.transformers = transformers\n",
    "\n",
    "    def fit(self, X):\n",
    "        X_transformed = np.array(X)\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            # Fit on current data\n",
    "            transformer.fit(X_transformed)\n",
    "            # Transform for next step\n",
    "            X_transformed = transformer.transform(X_transformed)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply all transformers seqeuntially\"\"\"\n",
    "        X_transformed = np.array(X)\n",
    "\n",
    "        for transformer in self.transformers:\n",
    "            # Transform data\n",
    "            X_transformed = transformer.transform(X_transformed)\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create randomize train and test data\n",
    "X, y = make_classification(\n",
    "    n_samples=20,\n",
    "    n_features=3,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Check shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a896913",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_list = [Log_Transformer(), Standard_Scaler()]\n",
    "pipeline = Pipeline(transformers_list)\n",
    "\n",
    "X_train_transformed = pipeline.fit_transform(abs(X_train))\n",
    "X_test_transformed = pipeline.transform(abs(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc82457",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Standard_Scaler()\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e9b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a06ff7e9",
   "metadata": {},
   "source": [
    "# Hashmaps to check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761012a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {\"id\": \"txn_001\", \"user_id\": \"U100\", \"amount\": 50.00, \"timestamp\": 1000},\n",
    "    {\"id\": \"txn_002\", \"user_id\": \"U100\", \"amount\": 50.00, \"timestamp\": 1003},\n",
    "    {\"id\": \"txn_003\", \"user_id\": \"U100\", \"amount\": 50.00, \"timestamp\": 1500},\n",
    "    {\"id\": \"txn_004\", \"user_id\": \"U200\", \"amount\": 75.00, \"timestamp\": 1001},\n",
    "    {\"id\": \"txn_005\", \"user_id\": \"U200\", \"amount\": 75.00, \"timestamp\": 1008},\n",
    "    {\"id\": \"txn_006\", \"user_id\": \"U100\", \"amount\": 50.00, \"timestamp\": 1006},\n",
    "]\n",
    "T = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd54a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates in transactions\n",
    "def check_transaction_duplicates(transactions: list, T: float) -> list:\n",
    "    duplicate_index = []\n",
    "\n",
    "    for i in range(len(transactions)):\n",
    "        for j in range(i + 1, len(transactions)):\n",
    "            txn1 = transactions[i]\n",
    "            txn2 = transactions[j]\n",
    "\n",
    "            # Check the 3 conditions\n",
    "            same_user = txn1[\"user_id\"] == txn2[\"user_id\"]\n",
    "            same_amount = txn1[\"amount\"] == txn2[\"amount\"]\n",
    "            within_time = abs(txn1[\"timestamp\"] - txn2[\"timestamp\"]) <= T\n",
    "\n",
    "            if same_user and same_amount and within_time:\n",
    "                duplicate_index.append((txn1[\"id\"], txn2[\"id\"]))\n",
    "\n",
    "    return duplicate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity is O(n^2)\n",
    "start_time = time.time()\n",
    "duplicate_index = check_transaction_duplicates(transactions, T)\n",
    "print(f\"Time taken: {time.time() - start_time}\")\n",
    "print(duplicate_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = defaultdict(list)\n",
    "\n",
    "for txn in transactions:\n",
    "    key = (txn[\"user_id\"], txn[\"amount\"])\n",
    "    groups[key].append(txn)\n",
    "\n",
    "display(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_transactions(\n",
    "    transactions: list[dict], T: int\n",
    ") -> list[tuple[str, str]]:\n",
    "    # Step 1: Group transactions by (user_id, amount)\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    for txn in transactions:\n",
    "        key = (txn[\"user_id\"], txn[\"amount\"])\n",
    "        groups[key].append(txn)\n",
    "\n",
    "    # Step 2: For each group, find duplicates\n",
    "    duplicates = []\n",
    "\n",
    "    for key, txn_list in groups.items():\n",
    "        txn_list.sort(key=lambda x: x[\"timestamp\"])\n",
    "\n",
    "        for i in range(len(txn_list)):\n",
    "            for j in range(i + 1, len(txn_list)):\n",
    "                time_diff = txn_list[j][\"timestamp\"] - txn_list[i][\"timestamp\"]\n",
    "\n",
    "                if time_diff > T:\n",
    "                    break\n",
    "\n",
    "                duplicates.append((txn_list[i][\"id\"], txn_list[j][\"id\"]))\n",
    "\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e829f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity is O(n)\n",
    "start_time = time.time()\n",
    "duplicates = find_duplicate_transactions(transactions, T)\n",
    "print(f\"Time taken: {time.time() - start_time}\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de655055",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = (\"txn_001\", \"txn_002\")\n",
    "assert duplicates[0] == expected, f\"Expected {expected}, got {duplicates[0]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a31066",
   "metadata": {},
   "source": [
    "# Price stream class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9fbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceStream:\n",
    "    def __init__(self):\n",
    "        self.prices = deque()\n",
    "        self.total_sum = 0\n",
    "        self.count = 0\n",
    "        self.max_deque = deque()\n",
    "        self.min_deque = deque()\n",
    "\n",
    "    # Add new price\n",
    "    def add(self, price) -> None:\n",
    "        if isinstance(price, (list, tuple)):\n",
    "            for p in price:\n",
    "                self._add_single(p)\n",
    "        else:\n",
    "            self._add_single(price)\n",
    "\n",
    "    # Add new price helper\n",
    "    def _add_single(self, price: float) -> None:\n",
    "        self.prices.append(price)\n",
    "        self.total_sum += price\n",
    "        self.count += 1\n",
    "\n",
    "        # Imnplement max_deque\n",
    "        while self.max_deque and self.max_deque[-1] < price:\n",
    "            self.max_deque.pop()\n",
    "        self.max_deque.append(price)\n",
    "\n",
    "        # Imnplement min_deque\n",
    "        while self.min_deque and self.min_deque[-1] > price:\n",
    "            self.min_deque.pop()\n",
    "        self.min_deque.append(price)\n",
    "\n",
    "    # Get max price from the current stream\n",
    "    def get_max(self) -> float:\n",
    "        return self.max_deque[0]\n",
    "\n",
    "    # Get min price from the current stream\n",
    "    def get_min(self) -> float:\n",
    "        return self.min_deque[0]\n",
    "\n",
    "    # Get mean price from the current steam\n",
    "    def get_mean(self) -> float:\n",
    "        return self.total_sum / self.count\n",
    "\n",
    "    # Remove oldest price in stream\n",
    "    def remove_oldest(self) -> list:\n",
    "        oldest_px = self.prices.popleft()\n",
    "        self.total_sum -= oldest_px\n",
    "        self.count -= 1\n",
    "\n",
    "        if self.max_deque and self.max_deque[0] == oldest_px:\n",
    "            self.prices.popleft()\n",
    "\n",
    "        if self.min_deque and self.min_deque[0] == oldest_px:\n",
    "            self.prices.popleft()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Current stream: {self.prices}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a858b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = PriceStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.add(10)\n",
    "stream.add(5)\n",
    "stream.add(8)\n",
    "stream.add(11)\n",
    "stream.add([random.randint(1, 50) for _ in range(10)])\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_px = stream.get_max()\n",
    "print(max_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_px = stream.get_min()\n",
    "print(min_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515655a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_px = stream.get_mean()\n",
    "print(mean_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.remove_oldest()\n",
    "print(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_px = stream.get_mean()\n",
    "print(mean_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872516a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_px = stream.get_min()\n",
    "print(min_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a95caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_px = stream.get_max()\n",
    "print(max_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefc739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
