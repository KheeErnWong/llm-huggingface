{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b15c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kheeern/AIAP/aiap/llm-huggingface/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8abacf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9992892742156982}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline is a high level API that wraps three steps:\n",
    "# Tokenization, model inference, and post-processing\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a hugging face course my whole life!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love to play poker, it is very fun!\", \"I hate doing household chores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc5062e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998818635940552},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995378255844116}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdace6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445961475372314, 0.11197615414857864, 0.04342764988541603]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd1c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ZeroShotClassificationPipeline in module transformers.pipelines.zero_shot_classification object:\n",
      "\n",
      "class ZeroShotClassificationPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  ZeroShotClassificationPipeline(\n",
      " |      args_parser=<transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler object at 0x147a5f620>,\n",
      " |      **kwargs\n",
      " |  )\n",
      " |\n",
      " |  NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification` trained on NLI (natural\n",
      " |  language inference) tasks. Equivalent of `text-classification` pipelines, but these models don't require a\n",
      " |  hardcoded number of potential classes, they can be chosen at runtime. It usually means it's slower but it is\n",
      " |  **much** more flexible.\n",
      " |\n",
      " |  Any combination of sequences and labels can be passed and each combination will be posed as a premise/hypothesis\n",
      " |  pair and passed to the pretrained model. Then, the logit for *entailment* is taken as the logit for the candidate\n",
      " |  label being valid. Any NLI model can be used, but the id of the *entailment* label must be included in the model\n",
      " |  config's :attr:*~transformers.PretrainedConfig.label2id*.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |\n",
      " |  >>> oracle = pipeline(model=\"facebook/bart-large-mnli\")\n",
      " |  >>> oracle(\n",
      " |  ...     \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
      " |  ...     candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
      " |  ... )\n",
      " |  {'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}\n",
      " |\n",
      " |  >>> oracle(\n",
      " |  ...     \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
      " |  ...     candidate_labels=[\"english\", \"german\"],\n",
      " |  ... )\n",
      " |  {'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['english', 'german'], 'scores': [0.814, 0.186]}\n",
      " |  ```\n",
      " |\n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |\n",
      " |  This NLI pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"zero-shot-classification\"`.\n",
      " |\n",
      " |  The models that this pipeline can use are models that have been fine-tuned on an NLI task. See the up-to-date list\n",
      " |  of available models on [huggingface.co/models](https://huggingface.co/models?search=nli).\n",
      " |\n",
      " |      Arguments:\n",
      " |          model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |              The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |              [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |          tokenizer ([`PreTrainedTokenizer`]):\n",
      " |              The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |              [`PreTrainedTokenizer`].\n",
      " |          modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |              Model card attributed to the model for this pipeline.\n",
      " |          framework (`str`, *optional*):\n",
      " |              The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |              installed.\n",
      " |\n",
      " |              If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |              both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |              provided.\n",
      " |          task (`str`, defaults to `\"\"`):\n",
      " |              A task-identifier for the pipeline.\n",
      " |          num_workers (`int`, *optional*, defaults to 8):\n",
      " |              When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |              workers to be used.\n",
      " |          batch_size (`int`, *optional*, defaults to 1):\n",
      " |              When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |              the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |              pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |          args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |              Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |          device (`int`, *optional*, defaults to -1):\n",
      " |              Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |              the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |          dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |              (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |          binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |              Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |              the raw output data e.g. text.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ZeroShotClassificationPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, sequences: Union[str, list[str]], *args, **kwargs)\n",
      " |      Classify the sequence(s) given as inputs. See the [`ZeroShotClassificationPipeline`] documentation for more\n",
      " |      information.\n",
      " |\n",
      " |      Args:\n",
      " |          sequences (`str` or `list[str]`):\n",
      " |              The sequence(s) to classify, will be truncated if the model input is too large.\n",
      " |          candidate_labels (`str` or `list[str]`):\n",
      " |              The set of possible class labels to classify each sequence into. Can be a single label, a string of\n",
      " |              comma-separated labels, or a list of labels.\n",
      " |          hypothesis_template (`str`, *optional*, defaults to `\"This example is {}.\"`):\n",
      " |              The template used to turn each label into an NLI-style hypothesis. This template must include a {} or\n",
      " |              similar syntax for the candidate label to be inserted into the template. For example, the default\n",
      " |              template is `\"This example is {}.\"` With the candidate label `\"sports\"`, this would be fed into the\n",
      " |              model like `\"<cls> sequence to classify <sep> This example is sports . <sep>\"`. The default template\n",
      " |              works well in many cases, but it may be worthwhile to experiment with different templates depending on\n",
      " |              the task setting.\n",
      " |          multi_label (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not multiple candidate labels can be true. If `False`, the scores are normalized such that\n",
      " |              the sum of the label likelihoods for each sequence is 1. If `True`, the labels are considered\n",
      " |              independent and probabilities are normalized for each candidate by doing a softmax of the entailment\n",
      " |              score vs. the contradiction score.\n",
      " |\n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |\n",
      " |          - **sequence** (`str`) -- The sequence for which this is the output.\n",
      " |          - **labels** (`list[str]`) -- The labels sorted by order of likelihood.\n",
      " |          - **scores** (`list[float]`) -- The probabilities for each of the labels.\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      args_parser=<transformers.pipelines.zero_shot_classification.ZeroShotClassificationArgumentHandler object at 0x147a5f620>,\n",
      " |      **kwargs\n",
      " |  )\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  postprocess(self, model_outputs, multi_label=False)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |\n",
      " |  preprocess(\n",
      " |      self,\n",
      " |      inputs,\n",
      " |      candidate_labels=None,\n",
      " |      hypothesis_template='This example is {}.'\n",
      " |  )\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  entailment_id\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |\n",
      " |  get_iterator(\n",
      " |      self,\n",
      " |      inputs,\n",
      " |      num_workers: int,\n",
      " |      batch_size: int,\n",
      " |      preprocess_params,\n",
      " |      forward_params,\n",
      " |      postprocess_params\n",
      " |  )\n",
      " |\n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          supported_models (`list[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |\n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |\n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |\n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |\n",
      " |      Return:\n",
      " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |\n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |\n",
      " |  get_inference_context(self)\n",
      " |\n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  push_to_hub(\n",
      " |      self,\n",
      " |      repo_id: str,\n",
      " |      use_temp_dir: Optional[bool] = None,\n",
      " |      commit_message: Optional[str] = None,\n",
      " |      private: Optional[bool] = None,\n",
      " |      token: Union[bool, str, NoneType] = None,\n",
      " |      max_shard_size: Union[str, int, NoneType] = '5GB',\n",
      " |      create_pr: bool = False,\n",
      " |      safe_serialization: bool = True,\n",
      " |      revision: Optional[str] = None,\n",
      " |      commit_description: Optional[str] = None,\n",
      " |      tags: Optional[list[str]] = None,\n",
      " |      **deprecated_kwargs\n",
      " |  ) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the ðŸ¤— Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |\n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  save_pretrained(\n",
      " |      self,\n",
      " |      save_directory: Union[str, os.PathLike],\n",
      " |      safe_serialization: bool = True,\n",
      " |      **kwargs: Any\n",
      " |  )\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  dtype\n",
      " |      Dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  default_input_names = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71f27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
